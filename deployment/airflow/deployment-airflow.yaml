apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow
  labels:
    app: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
  template:
    metadata:
      labels:
        app: airflow
    spec:
      securityContext:
        runAsUser: 50000 # run as airflow user
        runAsGroup: 0
        # fsGroup: 50000      # uncomment if you want group permissions on volumes
      initContainers:
      - name: init-airflow-db
        image: osclimate/airflow:3.0.3
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
        command: [ "/bin/bash", "-c" ]

        args:
        - |
          if [[ -z "${AIRFLOW_UID}" ]]; then
            echo
            echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
            echo "If you are on Linux, you SHOULD follow the instructions below to set "
            echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
            echo "For other operating systems you can get rid of the warning with manually created .env file:"
            echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
            echo
            export AIRFLOW_UID=$(id -u)
          fi
          one_meg=1048576
          mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
          cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
          disk_available=$$(df / | tail -1 | awk '{print $$4}')
          warning_resources="false"
          if (( mem_available < 4000 )) ; then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
            echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
            echo
            warning_resources="true"
          fi
          if (( cpus_available < 2 )); then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
            echo "At least 2 CPUs recommended. You have $${cpus_available}"
            echo
            warning_resources="true"
          fi
          if (( disk_available < one_meg * 10 )); then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
            echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
            echo
            warning_resources="true"
          fi
          if [[ $${warning_resources} == "true" ]]; then
            echo
            echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
            echo "Please follow the instructions to increase amount of resources available:"
            echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
            echo
          fi
          echo
          echo "Creating missing opt dirs if missing:"
          echo
          mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
          echo
          echo "Airflow version:"
          /entrypoint airflow version
          echo
          echo "Files in shared volumes:"
          echo
          ls -la /opt/airflow/{logs,dags,plugins,config}
          echo
          echo "Running airflow config list to create default config file if missing."
          echo
          /entrypoint airflow config list >/dev/null
          echo
          echo "Files in shared volumes:"
          echo
          ls -la /opt/airflow/{logs,dags,plugins,config}
          echo
          echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
          echo
          chown -R "${AIRFLOW_UID}:0" /opt/airflow/
          echo
          echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
          echo
          chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
          echo
          echo "Files in shared volumes:"
          echo
          ls -la /opt/airflow/{logs,dags,plugins,config}
          # airflow db migrate && \
          # airflow users create \
          #   --username "$_AIRFLOW_WWW_USER_USERNAME" \
          #   --password "$_AIRFLOW_WWW_USER_PASSWORD" \
          #   --firstname "$_AIRFLOW_WWW_USER_FIRSTNAME" \
          #   --lastname "$_AIRFLOW_WWW_USER_LASTNAME" \
          #   --role "$_AIRFLOW_WWW_USER_ROLE" \
          #   --email "$_AIRFLOW_WWW_USER_EMAIL"
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          # value: KubernetesExecutor
          value: LocalExecutor
        # - name: AIRFLOW__WEBSERVER__SECRET_KEY
        - name: AIRFLOW__API__SECRET_KEY
          value: c3VwZXItc2VjdXJlLXNlY3JldC1rZXk=
        # - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        #   value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW_CONN_S3
          value: aws://minioadmin:minioadmin@/?endpoint_url=http:%2F%2Fminio:9000
        - name: AIRFLOW_CONN_TRINO_CONNECTION
          value: trino://admin:@trino:8080/
        - name: _AIRFLOW_DB_MIGRATE
          value: "true"
        - name: _AIRFLOW_WWW_USER_CREATE
          value: "true"
        - name: _AIRFLOW_WWW_USER_USERNAME
          value: airflow
        - name: _AIRFLOW_WWW_USER_PASSWORD
          value: airflow
        - name: _AIRFLOW_WWW_USER_ROLE
          value: Admin
        - name: _AIRFLOW_WWW_USER_FIRSTNAME
          value: Airflow
        - name: _AIRFLOW_WWW_USER_LASTNAME
          value: Admin
        - name: _AIRFLOW_WWW_USER_EMAIL
          value: airflowadmin@example.com
        - name: AIRFLOW__CORE__AUTH_MANAGER
          value: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
        - name: AIRFLOW__LOGGING__LOGGING_LEVEL
          value: INFO
        - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          value: 'true'
        - name: AIRFLOW__LOGGING__REMOTE_LOGGING
          value: "False"
        - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
          value: "/opt/airflow/logs"
        - name: AIRFLOW__API__AUTH_BACKENDS
          value: airflow.api.auth.backend.default
        - name: AIRFLOW__API__ACCESS_TOKEN_SECRET
          value: redhat2025$
        - name: AIRFLOW__CORE__AUTH_BACKEND
          value: airflow.api.auth.backend.jwt_auth
        - name: AIRFLOW__WEBSERVER__RBAC
          value: "True"

      containers:
      - name: airflow-webserver
        image: osclimate/airflow:3.0.3
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
        # args:
        # - bash
        # - '-c'
        # - exec airflow api-server
        command: [ "/bin/bash", "-c" ]
        # args:
        # - |
        #   airflow db migrate && \
        #   airflow api-server
        args:
        - |
          airflow api-server
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          # value: KubernetesExecutor
          value: LocalExecutor
        # - name: AIRFLOW__WEBSERVER__SECRET_KEY
        - name: AIRFLOW__API__SECRET_KEY
          value: c3VwZXItc2VjdXJlLXNlY3JldC1rZXk=
        # - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        #   value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW_CONN_S3
          value: aws://minioadmin:minioadmin@/?endpoint_url=http:%2F%2Fminio:9000
        - name: AIRFLOW_CONN_TRINO_CONNECTION
          value: trino://admin:@trino:8080/
        - name: _AIRFLOW_DB_MIGRATE
          value: "true"
        - name: _AIRFLOW_WWW_USER_CREATE
          value: "true"
        - name: _AIRFLOW_WWW_USER_USERNAME
          value: airflow
        - name: _AIRFLOW_WWW_USER_PASSWORD
          value: airflow
        - name: _AIRFLOW_WWW_USER_ROLE
          value: Admin
        - name: _AIRFLOW_WWW_USER_FIRSTNAME
          value: Airflow
        - name: _AIRFLOW_WWW_USER_LASTNAME
          value: Admin
        - name: _AIRFLOW_WWW_USER_EMAIL
          value: airflowadmin@example.com
        - name: AIRFLOW__CORE__AUTH_MANAGER
          value: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
        - name: AIRFLOW__LOGGING__LOGGING_LEVEL
          value: INFO
        - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          value: 'true'

        # - name: AIRFLOW__SCHEDULER__USE_DAG_PROCESSOR
        #   value: "True"
        # - name: AIRFLOW__SCHEDULER__PARSE_DAGS_INTERVAL
        #   value: "30"
        # - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
        #   value: "30"

        - name: AIRFLOW__LOGGING__REMOTE_LOGGING
          value: "False"
        - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
          value: "/opt/airflow/logs"
        - name: AIRFLOW__API__AUTH_BACKENDS
          value: airflow.api.auth.backend.default
        - name: AIRFLOW__API__ACCESS_TOKEN_SECRET
          value: redhat2025$
        - name: AIRFLOW__CORE__AUTH_BACKEND
          value: airflow.api.auth.backend.jwt_auth
        - name: AIRFLOW__WEBSERVER__RBAC
          value: "True"
        # - name: AIRFLOW__CORE__REMOTE_LOG_CONN_ID
        #   value: "s3_minio"
        # - name: AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER
        #   value: "s3://airflow-logs"

        # Optional: tune logging
        # - name: AIRFLOW__LOGGING__ENCRYPT_S3_LOGS
        #   value: "false"
        # - name: AIRFLOW__LOGGING__LOGGING_LEVEL
        #   value: "INFO"

        # # Define the MinIO connection string
        # - name: AIRFLOW_CONN_S3_MINIO
        #   value: "s3://minioadmin:minioadmin@minio.minio.svc.cluster.local:9000"
        ports:
        - containerPort: 8080

      - name: scheduler
        image: osclimate/airflow:3.0.3
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
        args:
        - bash
        - '-c'
        - exec airflow scheduler
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__API__SECRET_KEY
          value: c3VwZXItc2VjdXJlLXNlY3JldC1rZXk=
        # - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        #   value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW_CONN_S3
          value: aws://minioAdmin:minio1234/?endpoint_url=http:%2F%2Fminio:9000
        - name: AIRFLOW_CONN_TRINO_CONNECTION
          value: trino://admin:@trino:8080/
        # - name: _PIP_ADDITIONAL_REQUIREMENTS
        #   value: apache-airflow-providers-trino
        - name: _AIRFLOW_WWW_USER_USERNAME
          value: airflow
        - name: _AIRFLOW_WWW_USER_PASSWORD
          value: airflow
        - name: AIRFLOW__LOGGING__LOGGING_LEVEL
          value: DEBUG
        - name: AIRFLOW__CORE__DAGS_FOLDER
          value: /opt/airflow/dags
        # - name: AIRFLOW__SCHEDULER__USE_DAG_PROCESSOR
        #   value: "True"
        # - name: AIRFLOW__SCHEDULER__PARSE_DAGS_INTERVAL
        #   value: "30"
        # - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
        #   value: "30"
        - name: AIRFLOW__LOGGING__REMOTE_LOGGING
          value: "False"
        - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
          value: "/opt/airflow/logs"
        - name: AIRFLOW__API__AUTH_BACKENDS
          value: airflow.api.auth.backend.default
        - name: AIRFLOW__API__ACCESS_TOKEN_SECRET
          value: redhat2025$
        - name: AIRFLOW__WEBSERVER__RBAC
          value: "True"
        - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          value: 'true'
      # - name: triggerer
      #   image: osclimate/airflow:3.0.3
      #   securityContext:
      #     runAsUser: 50000
      #     runAsGroup: 0
      #   args:
      #   - bash
      #   - '-c'
      #   - exec airflow triggerer
      #   volumeMounts:
      #   - name: dags
      #     mountPath: /opt/airflow/dags
      #   - name: logs
      #     mountPath: /opt/airflow/logs
      #   env:
      #   - name: AIRFLOW__CORE__EXECUTOR
      #     value: "LocalExecutor"
      #   - name: AIRFLOW__API__SECRET_KEY
      #     value: c3VwZXItc2VjdXJlLXNlY3JldC1rZXk=
      #   - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
      #     value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
      #   - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
      #     value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
      #   - name: AIRFLOW__LOGGING__LOGGING_LEVEL
      #     value: DEBUG
      #   - name: AIRFLOW__CORE__DAGS_FOLDER
      #     value: /opt/airflow/dags
      #   # - name: AIRFLOW__SCHEDULER__USE_DAG_PROCESSOR
      #   #   value: "True"
      #   # - name: AIRFLOW__SCHEDULER__PARSE_DAGS_INTERVAL
      #   #   value: "30"
      #   # - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
      #   #   value: "30"
      #   - name: AIRFLOW__LOGGING__REMOTE_LOGGING
      #     value: "False"
      #   - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
      #     value: "/opt/airflow/logs"
      #   - name: AIRFLOW__API__AUTH_BACKENDS
      #     value: airflow.api.auth.backend.default
      #   - name: AIRFLOW__API__ACCESS_TOKEN_SECRET
      #     value: redhat2025$
      #   - name: AIRFLOW__CORE__AUTH_BACKEND
      #     value: airflow.api.auth.backend.jwt_auth
      #   - name: AIRFLOW__WEBSERVER__RBAC
      #     value: "True"
      #   - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
      #     value: 'true'
      - name: dag-processor
        image: osclimate/airflow:3.0.3
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
        command: [ "airflow" ]
        args: [ "dag-processor" ]
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__API__SECRET_KEY
          value: uCRF0Z6_xL3ScDW3UHVU8mcikwFxLcMKt5LFz5LRjIk
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123$@postgres:5432/airflow
        - name: AIRFLOW__CORE__DAGS_FOLDER
          value: /opt/airflow/dags
        - name: AIRFLOW__LOGGING__LOGGING_LEVEL
          value: DEBUG
        # - name: AIRFLOW__SCHEDULER__USE_DAG_PROCESSOR
        #   value: "True"
        # - name: AIRFLOW__SCHEDULER__PARSE_DAGS_INTERVAL
        #   value: "30"
        # - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
        #   value: "30"
        - name: AIRFLOW__LOGGING__REMOTE_LOGGING
          value: "False"
        - name: AIRFLOW__LOGGING__BASE_LOG_FOLDER
          value: "/opt/airflow/logs"
        - name: AIRFLOW__API__AUTH_BACKENDS
          value: airflow.api.auth.backend.default
        - name: AIRFLOW__API__ACCESS_TOKEN_SECRET
          value: redhat2025$
        - name: AIRFLOW__CORE__AUTH_BACKEND
          value: airflow.api.auth.backend.jwt_auth
        - name: AIRFLOW__WEBSERVER__RBAC
          value: "True"
        - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
          value: 'true'
      volumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc
      - name: logs
        persistentVolumeClaim:
          claimName: airflow-logs-pvc
      # volumes:
      # - name: dags
      #   hostPath:
      #     path: /dags
      #     type: Directory
